{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8415eb2-e33b-41a2-8768-c03f0f72e96c",
   "metadata": {},
   "source": [
    "### Prof. Fernando Amaral - www.eia.ai\n",
    "### Contribuição: Adriano Santos\n",
    "#### <strong><font color=orange>Machine Learning com Spark</font></strong>\n",
    "## <strong>`RobustScaler`</strong>\n",
    "\n",
    "- O `RobustScaler` do pacote scikit-learn em Python é usado para padronizar os dados, ou seja, transformá-los de modo que tenham uma média de zero e uma variância de um, a fim de facilitar a comparação e o processamento dos dados.\n",
    "\n",
    "- O principal benefício do `RobustScaler` em relação a outros métodos de padronização, como o StandardScaler, é sua robustez a **outliers**. O `RobustScaler` usa estatísticas resistentes a outliers, como a **mediana** e o **intervalo interquartil (IQR)**, para calcular a padronização dos dados. Isso torna o `RobustScaler` mais adequado para conjuntos de dados que contêm outliers ou distribuições não normais.\n",
    "\n",
    "- Além disso, o `RobustScaler` preserva a forma geral da distribuição dos dados, pois ele não assume que os dados são normalmente distribuídos. Isso pode ser útil quando a forma da distribuição é importante para a análise.\n",
    "\n",
    "- Em resumo, o `RobustScaler` serve para padronizar os dados, tornando-os mais comparáveis e processáveis, especialmente quando há **outliers** ou distribuições não normais nos dados.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5668ad",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **Biblioteca** `findspark` para localizar e inicializar o Spark no ambiente local. Em seguida, é importado o módulo `pyspark` e a **Classe** `SparkSession` do `pyspark.sql`. Com o `findspark.init()`, o Spark é inicializado, e uma instância da **Classe** `SparkSession` é criada com a função `builder()`, configurando o nome da aplicação como \"standardscaler\" através do **Parâmetro** `appName`. Essa sessão Spark é obtida ou criada com `getOrCreate()`, permitindo a execução de operações no cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69dfcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"standardscaler\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b16d5",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **Biblioteca** `pyspark.ml.feature` para importar a classe **RobustScaler**. O **RobustScaler** é uma **função** de pré-processamento que realiza a escalonamento robusto de características, tornando-as mais resistentes a `outliers`. Ele calcula a `mediana` e o `intervalo interquartil` (IQR) das características e as dimensiona de acordo, o que é particularmente útil em conjuntos de dados com valores discrepantes. Essa técnica é implementada através da classe **RobustScaler**, permitindo que os dados sejam transformados de forma robusta antes de serem utilizados em algoritmos de machine learning no ambiente do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d04c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a83432",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **Spark** para ler um arquivo CSV chamado \"Carros.csv\", localizado um diretório acima do diretório atual. A função **spark.read.csv()** é utilizada com os parâmetros para especificar o arquivo, definir se o arquivo possui um cabeçalho e inferir o esquema dos dados automaticamente. O parâmetro **sep=\";\"** indica que o separador de campos no arquivo CSV é o ponto e vírgula. Em seguida, a função **carros.show(5)** é chamada para exibir as primeiras cinco linhas do DataFrame resultante da leitura do arquivo CSV, permitindo uma visualização rápida dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23af824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|Consumo|Cilindros|Cilindradas|RelEixoTraseiro|Peso|Tempo|TipoMotor|Transmissao|Marchas|Carburadors| HP|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|     21|        6|        160|             39| 262| 1646|        0|          1|      4|          4|110|\n",
      "|     21|        6|        160|             39|2875| 1702|        0|          1|      4|          4|110|\n",
      "|    228|        4|        108|            385| 232| 1861|        1|          1|      4|          1| 93|\n",
      "|    214|        6|        258|            308|3215| 1944|        1|          0|      3|          1|110|\n",
      "|    187|        8|        360|            315| 344| 1702|        0|          0|      3|          2|175|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carros = spark.read.csv(\"../Carros.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "carros.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b51f84",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **biblioteca** `PySpark`, mais especificamente o módulo \"`feature`\", para importar a **Classe** `VectorAssembler`, responsável por combinar múltiplas colunas em uma única coluna de vetores. Em seguida, é instanciado um objeto vecassembler com os **parâmetros** de entrada (\"Consumo\", \"Cilindros\", \"Cilindradas\") e saída (\"Matriz\"). Posteriormente, a **função** `transform`() é chamada para aplicar a transformação aos dados do dataframe \"carros\". Por fim, a função `select`() é utilizada para exibir as colunas originais \"Consumo\", \"Cilindros\" e \"Cilindradas\", além da coluna resultante \"Matriz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991fb8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------------+\n",
      "|Consumo|Cilindros|Cilindradas|             vetor|\n",
      "+-------+---------+-----------+------------------+\n",
      "|     21|        6|        160|  [21.0,6.0,160.0]|\n",
      "|     21|        6|        160|  [21.0,6.0,160.0]|\n",
      "|    228|        4|        108| [228.0,4.0,108.0]|\n",
      "|    214|        6|        258| [214.0,6.0,258.0]|\n",
      "|    187|        8|        360| [187.0,8.0,360.0]|\n",
      "|    181|        6|        225| [181.0,6.0,225.0]|\n",
      "|    143|        8|        360| [143.0,8.0,360.0]|\n",
      "|    244|        4|       1467|[244.0,4.0,1467.0]|\n",
      "|    228|        4|       1408|[228.0,4.0,1408.0]|\n",
      "|    192|        6|       1676|[192.0,6.0,1676.0]|\n",
      "|    178|        6|       1676|[178.0,6.0,1676.0]|\n",
      "|    164|        8|       2758|[164.0,8.0,2758.0]|\n",
      "|    173|        8|       2758|[173.0,8.0,2758.0]|\n",
      "|    152|        8|       2758|[152.0,8.0,2758.0]|\n",
      "|    104|        8|        472| [104.0,8.0,472.0]|\n",
      "|    104|        8|        460| [104.0,8.0,460.0]|\n",
      "|    147|        8|        440| [147.0,8.0,440.0]|\n",
      "|    324|        4|        787| [324.0,4.0,787.0]|\n",
      "|    304|        4|        757| [304.0,4.0,757.0]|\n",
      "|    339|        4|        711| [339.0,4.0,711.0]|\n",
      "+-------+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecassembler = VectorAssembler(inputCols=[\"Consumo\",\"Cilindros\",\"Cilindradas\"], outputCol=\"vetor\")\n",
    "carros_vetor = vecassembler.transform(carros)\n",
    "\n",
    "carros_vetor.select(\"Consumo\",\"Cilindros\",\"Cilindradas\",\"vetor\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb41bd",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **RobustScaler**, uma classe da **biblioteca** de machine learning **PySpark**, para normalizar os dados de um conjunto de dados de carros. O parâmetro `inputCol` especifica a coluna de entrada contendo o vetor de características, enquanto `outputCol` indica o nome da coluna de saída para armazenar o vetor normalizado. Com `withScaling=True`, a função aplica o escalonamento, enquanto `withCentering=False` desabilita a centralização dos dados. Os parâmetros `lower` e `upper` definem os limites inferior e superior para o intervalo de escala, respectivamente. Após ajustar o scaler ao conjunto de dados com `escala.fit(carros_vetor)`, a transformação é aplicada aos dados originais usando a função `transform`, produzindo uma nova coluna denominada \"vetorscale\". Finalmente, os dados originais e normalizados são selecionados e exibidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6891b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------------------------------+\n",
      "|vetor             |vetorscale                                   |\n",
      "+------------------+---------------------------------------------+\n",
      "|[21.0,6.0,160.0]  |[0.29166666666666663,1.5,0.16967126193001061]|\n",
      "|[21.0,6.0,160.0]  |[0.29166666666666663,1.5,0.16967126193001061]|\n",
      "|[228.0,4.0,108.0] |[3.1666666666666665,1.0,0.11452810180275717] |\n",
      "|[214.0,6.0,258.0] |[2.972222222222222,1.5,0.27359490986214213]  |\n",
      "|[187.0,8.0,360.0] |[2.597222222222222,2.0,0.3817603393425239]   |\n",
      "|[181.0,6.0,225.0] |[2.513888888888889,1.5,0.23860021208907745]  |\n",
      "|[143.0,8.0,360.0] |[1.986111111111111,2.0,0.3817603393425239]   |\n",
      "|[244.0,4.0,1467.0]|[3.388888888888889,1.0,1.555673382820785]    |\n",
      "|[228.0,4.0,1408.0]|[3.1666666666666665,1.0,1.4931071049840934]  |\n",
      "|[192.0,6.0,1676.0]|[2.6666666666666665,1.5,1.7773064687168612]  |\n",
      "|[178.0,6.0,1676.0]|[2.472222222222222,1.5,1.7773064687168612]   |\n",
      "|[164.0,8.0,2758.0]|[2.2777777777777777,2.0,2.924708377518558]   |\n",
      "|[173.0,8.0,2758.0]|[2.4027777777777777,2.0,2.924708377518558]   |\n",
      "|[152.0,8.0,2758.0]|[2.111111111111111,2.0,2.924708377518558]    |\n",
      "|[104.0,8.0,472.0] |[1.4444444444444444,2.0,0.5005302226935313]  |\n",
      "|[104.0,8.0,460.0] |[1.4444444444444444,2.0,0.48780487804878053] |\n",
      "|[147.0,8.0,440.0] |[2.0416666666666665,2.0,0.4665959703075292]  |\n",
      "|[324.0,4.0,787.0] |[4.5,1.0,0.8345705196182397]                 |\n",
      "|[304.0,4.0,757.0] |[4.222222222222222,1.0,0.8027571580063627]   |\n",
      "|[339.0,4.0,711.0] |[4.708333333333333,1.0,0.7539766702014847]   |\n",
      "+------------------+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "escala = RobustScaler(inputCol=\"vetor\", outputCol=\"vetorscale\", withScaling=True, withCentering=False, lower=0.25, upper=0.75)\n",
    "modelo = escala.fit(carros_vetor)\n",
    "carrosstandard = modelo.transform(carros_vetor)\n",
    "\n",
    "carrosstandard.select(\"vetor\",\"vetorscale\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0db99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
