{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255eca5b-ed31-48bc-a6f8-7a937d2aeca6",
   "metadata": {},
   "source": [
    "### Prof. Fernando Amaral - www.eia.ai\n",
    "### Contribuição: Adriano Santos\n",
    "#### <strong><font color=orange>Machine Learning com Spark</font></strong>\n",
    "## <strong>`MinMaxScaler`</strong>\n",
    "O `MinMaxScaler` é uma técnica de pré-processamento de dados utilizada em aprendizado de máquina e análise de dados para normalizar valores numéricos dentro de um intervalo específico, geralmente 0 a 1 ou -1 a 1. Essa normalização é realizada subtraindo-se o valor mínimo de cada coluna e dividindo pelo seu range. `A principal função desse método` é garantir que todas as características do conjunto de dados estejam na mesma escala, o que é essencial para a maioria dos algoritmos de aprendizado de máquina. Além disso, a normalização pode melhorar a eficiência desses algoritmos, facilitar a interpretação e comparação dos dados, e reduzir o ruído. <br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968ffa5",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a biblioteca **findspark** para localizar e iniciar o ambiente Spark e a biblioteca **pyspark** para trabalhar com Spark em Python. Primeiro, a função **init()** da **findspark** é chamada para configurar o ambiente. Em seguida, uma sessão Spark é criada usando a função **builder.appName(\"standardscaler\").getOrCreate()** da classe **SparkSession**. Essa sessão é nomeada \"standardscaler\" e é essencial para executar operações de análise de dados com PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631db41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"standardscaler\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e017823",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **biblioteca** `pyspark.ml.feature` para importar a **classe** `MinMaxScaler`, que é usada para normalizar os dados em um intervalo específico. A **função** `MinMaxScaler` transforma os dados, ajustando os valores para que estejam dentro de um intervalo definido, geralmente entre 0 e 1, o que é útil para melhorar o desempenho de algoritmos de aprendizado de máquina. Ao aplicar o **MinMaxScaler**, os valores mínimos e máximos dos dados são ajustados conforme os parâmetros definidos, e os demais valores são escalonados proporcionalmente dentro deste intervalo. Isso é particularmente útil para tratar problemas de escala e garantir que todas as características dos dados tenham o mesmo peso, evitando que variáveis com maiores magnitudes dominem as análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a562a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51bcc0",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o *framework* **Spark** para leitura de um arquivo CSV contendo informações sobre carros. O arquivo é lido utilizando a **função** *read.csv* da *biblioteca* **Spark**, onde os parâmetros especificados são: o caminho para o arquivo (\"../Carros.csv\"), *header=True* para indicar que a primeira linha do arquivo contém os nomes das colunas, *inferSchema=True* para que o Spark deduza automaticamente o tipo de dado de cada coluna, e *sep=\";\"* para indicar que o separador de campo no arquivo CSV é o ponto e vírgula. Após a leitura do arquivo, os dados são armazenados na variável **carros**. A **função** *show(5)* é então chamada nesta variável para exibir as primeiras cinco linhas do DataFrame resultante, permitindo uma visualização inicial dos dados carregados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fdb9e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|Consumo|Cilindros|Cilindradas|RelEixoTraseiro|Peso|Tempo|TipoMotor|Transmissao|Marchas|Carburadors| HP|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|     21|        6|        160|             39| 262| 1646|        0|          1|      4|          4|110|\n",
      "|     21|        6|        160|             39|2875| 1702|        0|          1|      4|          4|110|\n",
      "|    228|        4|        108|            385| 232| 1861|        1|          1|      4|          1| 93|\n",
      "|    214|        6|        258|            308|3215| 1944|        1|          0|      3|          1|110|\n",
      "|    187|        8|        360|            315| 344| 1702|        0|          0|      3|          2|175|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carros = spark.read.csv(\"../Carros.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "carros.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6f990",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado a **biblioteca** `pyspark.ml.feature` para importar a **classe** `VectorAssembler`. Inicialmente, cria-se uma instância dessa classe chamada `vecassembler`, onde são definidos os **parâmetros** `inputCols` e `outputCol`. O **parâmetro** `inputCols` recebe uma lista com os nomes das colunas \"Consumo\", \"Cilindros\" e \"Cilindradas\", enquanto `outputCol` especifica o nome da nova coluna que será criada, neste caso, \"vetor\". Em seguida, aplica-se a **função** `transform` ao DataFrame `carros` para gerar um novo DataFrame `carros_vetor`, que inclui a coluna \"vetor\" contendo os valores das colunas especificadas combinados em um vetor. Finalmente, o código seleciona e exibe as colunas \"Consumo\", \"Cilindros\", \"Cilindradas\" e \"vetor\" usando a **função** `select` seguida de `show`, mostrando assim o resultado transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897710fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------------+\n",
      "|Consumo|Cilindros|Cilindradas|             vetor|\n",
      "+-------+---------+-----------+------------------+\n",
      "|     21|        6|        160|  [21.0,6.0,160.0]|\n",
      "|     21|        6|        160|  [21.0,6.0,160.0]|\n",
      "|    228|        4|        108| [228.0,4.0,108.0]|\n",
      "|    214|        6|        258| [214.0,6.0,258.0]|\n",
      "|    187|        8|        360| [187.0,8.0,360.0]|\n",
      "|    181|        6|        225| [181.0,6.0,225.0]|\n",
      "|    143|        8|        360| [143.0,8.0,360.0]|\n",
      "|    244|        4|       1467|[244.0,4.0,1467.0]|\n",
      "|    228|        4|       1408|[228.0,4.0,1408.0]|\n",
      "|    192|        6|       1676|[192.0,6.0,1676.0]|\n",
      "|    178|        6|       1676|[178.0,6.0,1676.0]|\n",
      "|    164|        8|       2758|[164.0,8.0,2758.0]|\n",
      "|    173|        8|       2758|[173.0,8.0,2758.0]|\n",
      "|    152|        8|       2758|[152.0,8.0,2758.0]|\n",
      "|    104|        8|        472| [104.0,8.0,472.0]|\n",
      "|    104|        8|        460| [104.0,8.0,460.0]|\n",
      "|    147|        8|        440| [147.0,8.0,440.0]|\n",
      "|    324|        4|        787| [324.0,4.0,787.0]|\n",
      "|    304|        4|        757| [304.0,4.0,757.0]|\n",
      "|    339|        4|        711| [339.0,4.0,711.0]|\n",
      "+-------+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecassembler = VectorAssembler(inputCols=[\"Consumo\",\"Cilindros\",\"Cilindradas\"], outputCol=\"vetor\")\n",
    "carros_vetor = vecassembler.transform(carros)\n",
    "carros_vetor.select(\"Consumo\",\"Cilindros\",\"Cilindradas\",\"vetor\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7fc96",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado a **biblioteca** PySpark para normalizar os dados de um conjunto chamado `carros_vetor`. Primeiro, uma instância da **classe** `MinMaxScaler` é criada com os **parâmetros** `inputCol` (coluna de entrada), `outputCol` (coluna de saída), `min` (valor mínimo), e `max` (valor máximo), onde `inputCol` é definido como \"vetor\" e `outputCol` como \"minmaxscale\", com valores mínimo e máximo de 0 e 1, respectivamente. Em seguida, a **função** `fit` é aplicada sobre o conjunto de dados `carros_vetor` para ajustar o escalador aos dados, resultando no modelo `modelo`. Esse modelo é então usado para transformar os dados originais com a **função** `transform`, gerando um novo conjunto de dados chamado `carrosminmax`. Finalmente, as colunas \"vetor\" e \"minmaxscale\" do conjunto `carrosminmax` são selecionadas e exibidas sem truncamento de dados com a **função** `show(truncate=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418241bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------+\n",
      "|vetor             |minmaxscale                                    |\n",
      "+------------------+-----------------------------------------------+\n",
      "|[21.0,6.0,160.0]  |[0.018518518518518517,0.5,0.030235162374020158]|\n",
      "|[21.0,6.0,160.0]  |[0.018518518518518517,0.5,0.030235162374020158]|\n",
      "|[228.0,4.0,108.0] |[0.6574074074074073,0.0,0.010824934677118328]  |\n",
      "|[214.0,6.0,258.0] |[0.6141975308641975,0.5,0.06681597611048899]   |\n",
      "|[187.0,8.0,360.0] |[0.5308641975308641,1.0,0.10488988428518103]   |\n",
      "|[181.0,6.0,225.0] |[0.5123456790123456,0.5,0.05449794699514744]   |\n",
      "|[143.0,8.0,360.0] |[0.3950617283950617,1.0,0.10488988428518103]   |\n",
      "|[244.0,4.0,1467.0]|[0.70679012345679,0.0,0.5181037700634565]      |\n",
      "|[228.0,4.0,1408.0]|[0.6574074074074073,0.0,0.49608062709966405]   |\n",
      "|[192.0,6.0,1676.0]|[0.5462962962962963,0.5,0.5961179544606197]    |\n",
      "|[178.0,6.0,1676.0]|[0.5030864197530864,0.5,0.5961179544606197]    |\n",
      "|[164.0,8.0,2758.0]|[0.45987654320987653,1.0,1.0]                  |\n",
      "|[173.0,8.0,2758.0]|[0.48765432098765427,1.0,1.0]                  |\n",
      "|[152.0,8.0,2758.0]|[0.4228395061728395,1.0,1.0]                   |\n",
      "|[104.0,8.0,472.0] |[0.27469135802469136,1.0,0.14669652855543114]  |\n",
      "|[104.0,8.0,460.0] |[0.27469135802469136,1.0,0.1422172452407615]   |\n",
      "|[147.0,8.0,440.0] |[0.4074074074074074,1.0,0.1347517730496454]    |\n",
      "|[324.0,4.0,787.0] |[0.9537037037037036,0.0,0.2642777155655095]    |\n",
      "|[304.0,4.0,757.0] |[0.8919753086419753,0.0,0.2530795072788354]    |\n",
      "|[339.0,4.0,711.0] |[1.0,0.0,0.23590892123926838]                  |\n",
      "+------------------+-----------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "escala = MinMaxScaler(inputCol=\"vetor\", outputCol=\"minmaxscale\", min=0, max=1)\n",
    "modelo = escala.fit(carros_vetor)\n",
    "carrosminmax = modelo.transform(carros_vetor)\n",
    "carrosminmax.select(\"vetor\",\"minmaxscale\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab0920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
