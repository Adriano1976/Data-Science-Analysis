{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5169fe-a139-4953-a527-0f89138a811e",
   "metadata": {},
   "source": [
    "### Prof. Fernando Amaral - www.eia.ai\n",
    "### Contribuição: Adriano Santos\n",
    "#### <strong><font color=orange>Machine Learning com Spark</font></strong>\n",
    "## <strong>`RFormula`</strong>\n",
    "`RFormula` é uma classe no Apache Spark MLlib que permite ***definir um formulário de entrada para modelos de aprendizado de máquina***, especificando colunas categóricas e numéricas. Ele é usado principalmente para ***preparar dados antes de treinar modelos de machine learning***, permitindo que os usuários especifiquem quais colunas devem ser tratadas como características (numéricas) e quais como rótulos (categóricas). Isso ajuda a garantir que o modelo seja treinado corretamente, considerando o tipo de dados de cada coluna.\n",
    "</br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ce056",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **módulo** `findspark` para inicializar o ambiente do Apache Spark dentro do Python. Em seguida, a **classe** `SparkSession` da **biblioteca** `pyspark.sql` é usada para criar uma sessão Spark com o nome de aplicação \"rfomula\". Essa sessão Spark permite a interação com o cluster Spark, facilitando a execução de tarefas distribuídas e processamento de grandes volumes de dados. Além disso, é importada a **classe** `RFormula` da **biblioteca** `pyspark.ml.feature`, que é uma ferramenta útil para transformar dados em um formato adequado para aprendizado de máquina, similar ao R. A função `findspark.init()` é chamada para garantir que o Spark esteja configurado corretamente antes de iniciar a sessão Spark. Por fim, `SparkSession.builder.appName(\"rfomula\").getOrCreate()` cria ou obtém uma sessão Spark, essencial para operações subsequentes de processamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"rfomula\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcf221",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o *Spark* para ler um arquivo CSV chamado \"Carros.csv\" localizado no diretório superior ao do script. O *Spark* é uma *biblioteca* utilizada para processamento de dados em grande escala. A função **`spark.read.csv`** é utilizada para ler o arquivo CSV, com os parâmetros **`header=True`**, que indica que a primeira linha do arquivo contém os cabeçalhos das colunas, **`inferSchema=True`**, que permite ao Spark inferir automaticamente o tipo de dados de cada coluna, e **`sep=\";\"`**, que especifica que o delimitador de campo no arquivo CSV é o ponto e vírgula. Após a leitura, o *DataFrame* resultante é armazenado na variável **`carros`**. Em seguida, a função **`carros.show(5)`** é chamada para exibir as primeiras cinco linhas do *DataFrame* **`carros`**, permitindo uma visualização rápida dos dados importados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd1950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|Consumo|Cilindros|Cilindradas|RelEixoTraseiro|Peso|Tempo|TipoMotor|Transmissao|Marchas|Carburadors| HP|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|     21|        6|        160|             39| 262| 1646|        0|          1|      4|          4|110|\n",
      "|     21|        6|        160|             39|2875| 1702|        0|          1|      4|          4|110|\n",
      "|    228|        4|        108|            385| 232| 1861|        1|          1|      4|          1| 93|\n",
      "|    214|        6|        258|            308|3215| 1944|        1|          0|      3|          1|110|\n",
      "|    187|        8|        360|            315| 344| 1702|        0|          0|      3|          2|175|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carros = spark.read.csv(\"../Carros.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "carros.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403141b7",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **biblioteca** `pyspark.ml.feature` para aplicar a **classe** `RFormula`, que facilita a criação de **features** (variáveis independentes) e **labels** (variáveis dependentes) a partir de uma fórmula no estilo R. O **parâmetro** `formula` é utilizado para definir a relação entre as variáveis, onde \"HP\" é a variável dependente e \"Consumo\", \"Cilindros\" e \"Cilindradas\" são as variáveis independentes. O **parâmetro** `featuresCol` é definido como \"independente\" e `labelCol` como \"dependente\". Em seguida, a **função** `fit` da **classe** `RFormula` é aplicada ao DataFrame `carros` para ajustar o modelo e, posteriormente, a **função** `transform` é utilizada para transformar o DataFrame original adicionando as colunas \"independente\" e \"dependente\". Por fim, o método `select` é chamado para selecionar e exibir as colunas \"independente\" e \"dependente\" do DataFrame resultante, utilizando a **função** `show` para mostrar os dados na saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fd67b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|      independente|dependente|\n",
      "+------------------+----------+\n",
      "|  [21.0,6.0,160.0]|     110.0|\n",
      "|  [21.0,6.0,160.0]|     110.0|\n",
      "| [228.0,4.0,108.0]|      93.0|\n",
      "| [214.0,6.0,258.0]|     110.0|\n",
      "| [187.0,8.0,360.0]|     175.0|\n",
      "| [181.0,6.0,225.0]|     105.0|\n",
      "| [143.0,8.0,360.0]|     245.0|\n",
      "|[244.0,4.0,1467.0]|      62.0|\n",
      "|[228.0,4.0,1408.0]|      95.0|\n",
      "|[192.0,6.0,1676.0]|     123.0|\n",
      "|[178.0,6.0,1676.0]|     123.0|\n",
      "|[164.0,8.0,2758.0]|     180.0|\n",
      "|[173.0,8.0,2758.0]|     180.0|\n",
      "|[152.0,8.0,2758.0]|     180.0|\n",
      "| [104.0,8.0,472.0]|     205.0|\n",
      "| [104.0,8.0,460.0]|     215.0|\n",
      "| [147.0,8.0,440.0]|     230.0|\n",
      "| [324.0,4.0,787.0]|      66.0|\n",
      "| [304.0,4.0,757.0]|      52.0|\n",
      "| [339.0,4.0,711.0]|      65.0|\n",
      "+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Rformula = RFormula(formula=\"HP ~ Consumo + Cilindros + Cilindradas\", featuresCol=\"independente\", labelCol=\"dependente\")\n",
    "carrosrf = Rformula.fit(carros).transform(carros)\n",
    "\n",
    "carrosrf.select(\"independente\", \"dependente\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2129e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
