{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6f2072-7686-4441-bdf8-28acd1c2b378",
   "metadata": {},
   "source": [
    "### Prof. Fernando Amaral - www.eia.ai\n",
    "### Contribuição: Adriano Santos\n",
    "#### <strong><font color=orange>Machine Learning com Spark</font></strong>\n",
    "## <strong>`StandardScaler`</strong>\n",
    "- Para normalizar os atributos para o desvio padrão ou média zero, podemos utilizar a biblioteca Scikit-learn em Python. Para realizar a normalização com desvio padrão (withStd=True), podemos utilizar a classe StandardScaler. Essa classe padroniza os recursos removendo a média e escalando para a variância unitária. \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73939ad6",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **Biblioteca** `findspark` para localizar e inicializar o Spark no ambiente local. Em seguida, é importado o módulo `pyspark` e a **Classe** `SparkSession` do `pyspark.sql`. Com o `findspark.init()`, o Spark é inicializado, e uma instância da **Classe** `SparkSession` é criada com a função `builder()`, configurando o nome da aplicação como \"standardscaler\" através do **Parâmetro** `appName`. Essa sessão Spark é obtida ou criada com `getOrCreate()`, permitindo a execução de operações no cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe1959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"standardscaler\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4b541",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo importada a **Biblioteca** `StandardScaler` do `pyspark.ml.feature`, a qual fornece funcionalidades para pré-processamento de dados em Spark. Essa biblioteca oferece uma classe chamada **StandardScaler**, a qual é utilizada para padronizar os recursos de um conjunto de dados, garantindo que tenham média zero e desvio padrão um, o que é essencial para muitos algoritmos de aprendizado de máquina. Essa padronização é realizada através de uma **Função** que calcula as médias e os desvios padrão dos recursos e, em seguida, aplica a transformação aos dados, resultando em um conjunto de dados escalonado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55db1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b3e48",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **Spark** para ler um arquivo CSV chamado \"Carros.csv\", localizado um diretório acima do diretório atual. A função **spark.read.csv()** é utilizada com os parâmetros para especificar o arquivo, definir se o arquivo possui um cabeçalho e inferir o esquema dos dados automaticamente. O parâmetro **sep=\";\"** indica que o separador de campos no arquivo CSV é o ponto e vírgula. Em seguida, a função **carros.show(5)** é chamada para exibir as primeiras cinco linhas do DataFrame resultante da leitura do arquivo CSV, permitindo uma visualização rápida dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35bff019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|Consumo|Cilindros|Cilindradas|RelEixoTraseiro|Peso|Tempo|TipoMotor|Transmissao|Marchas|Carburadors| HP|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "|     21|        6|        160|             39| 262| 1646|        0|          1|      4|          4|110|\n",
      "|     21|        6|        160|             39|2875| 1702|        0|          1|      4|          4|110|\n",
      "|    228|        4|        108|            385| 232| 1861|        1|          1|      4|          1| 93|\n",
      "|    214|        6|        258|            308|3215| 1944|        1|          0|      3|          1|110|\n",
      "|    187|        8|        360|            315| 344| 1702|        0|          0|      3|          2|175|\n",
      "+-------+---------+-----------+---------------+----+-----+---------+-----------+-------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carros = spark.read.csv(\"../Carros.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "carros.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb8957",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **biblioteca** `PySpark`, mais especificamente o módulo \"`feature`\", para importar a **Classe** `VectorAssembler`, responsável por combinar múltiplas colunas em uma única coluna de vetores. Em seguida, é instanciado um objeto vecassembler com os **parâmetros** de entrada (\"Consumo\", \"Cilindros\", \"Cilindradas\") e saída (\"Matriz\"). Posteriormente, a **função** `transform`() é chamada para aplicar a transformação aos dados do dataframe \"carros\". Por fim, a função `select`() é utilizada para exibir as colunas originais \"Consumo\", \"Cilindros\" e \"Cilindradas\", além da coluna resultante \"Matriz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe772a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------------+\n",
      "|Consumo|Cilindros|Cilindradas|            Matriz|\n",
      "+-------+---------+-----------+------------------+\n",
      "|     21|        6|        160|  [21.0,6.0,160.0]|\n",
      "|     21|        6|        160|  [21.0,6.0,160.0]|\n",
      "|    228|        4|        108| [228.0,4.0,108.0]|\n",
      "|    214|        6|        258| [214.0,6.0,258.0]|\n",
      "|    187|        8|        360| [187.0,8.0,360.0]|\n",
      "|    181|        6|        225| [181.0,6.0,225.0]|\n",
      "|    143|        8|        360| [143.0,8.0,360.0]|\n",
      "|    244|        4|       1467|[244.0,4.0,1467.0]|\n",
      "|    228|        4|       1408|[228.0,4.0,1408.0]|\n",
      "|    192|        6|       1676|[192.0,6.0,1676.0]|\n",
      "|    178|        6|       1676|[178.0,6.0,1676.0]|\n",
      "|    164|        8|       2758|[164.0,8.0,2758.0]|\n",
      "|    173|        8|       2758|[173.0,8.0,2758.0]|\n",
      "|    152|        8|       2758|[152.0,8.0,2758.0]|\n",
      "|    104|        8|        472| [104.0,8.0,472.0]|\n",
      "|    104|        8|        460| [104.0,8.0,460.0]|\n",
      "|    147|        8|        440| [147.0,8.0,440.0]|\n",
      "|    324|        4|        787| [324.0,4.0,787.0]|\n",
      "|    304|        4|        757| [304.0,4.0,757.0]|\n",
      "|    339|        4|        711| [339.0,4.0,711.0]|\n",
      "+-------+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecassembler = VectorAssembler(inputCols=[\"Consumo\",\"Cilindros\",\"Cilindradas\"], outputCol=\"Matriz\")\n",
    "carros_vetor = vecassembler.transform(carros)\n",
    "\n",
    "carros_vetor.select(\"Consumo\",\"Cilindros\",\"Cilindradas\",\"Matriz\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1261e",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizada a **Biblioteca** `StandardScaler` do **Python**, que implementa a padronização de recursos para uma matriz de dados, com a criação de uma instância chamada `escala`. Essa instância é configurada com parâmetros específicos, como a coluna de entrada (`inputCol`) e de saída (`outputCol`). Em seguida, é criado um **objeto** `modelo`, o qual é ajustado aos dados `carros_vetor` por meio do método `fit()` da instância `escala`. Posteriormente, os dados são transformados usando o método `transform()` do modelo ajustado, resultando em `carrosstandard`, que contém as colunas originais e as colunas padronizadas. Por fim, uma visualização dos dados é exibida usando o método `show`, mostrando as colunas \"Matriz\" e \"standard\" sem truncamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffa0f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------------------+\n",
      "|Matriz            |standard                                                   |\n",
      "+------------------+-----------------------------------------------------------+\n",
      "|[21.0,6.0,160.0]  |[0.24996122082808128,3.359609874407659,0.20137542427273997]|\n",
      "|[21.0,6.0,160.0]  |[0.24996122082808128,3.359609874407659,0.20137542427273997]|\n",
      "|[228.0,4.0,108.0] |[2.713864683276311,2.239739916271773,0.13592841138409947]  |\n",
      "|[214.0,6.0,258.0] |[2.5472238693909235,3.359609874407659,0.32471787163979315] |\n",
      "|[187.0,8.0,360.0] |[2.2258451568976763,4.479479832543546,0.4530947046136649]  |\n",
      "|[181.0,6.0,225.0] |[2.15442766523251,3.359609874407659,0.28318419038354053]   |\n",
      "|[143.0,8.0,360.0] |[1.7021168846864583,4.479479832543546,0.4530947046136649]  |\n",
      "|[244.0,4.0,1467.0]|[2.904311327716754,2.239739916271773,1.8463609213006844]   |\n",
      "|[228.0,4.0,1408.0]|[2.713864683276311,2.239739916271773,1.7721037336001115]   |\n",
      "|[192.0,6.0,1676.0]|[2.2853597332853144,3.359609874407659,2.109407569256951]   |\n",
      "|[178.0,6.0,1676.0]|[2.118718919399927,3.359609874407659,2.109407569256951]    |\n",
      "|[164.0,8.0,2758.0]|[1.9520781055145395,4.479479832543546,3.471208875901355]   |\n",
      "|[173.0,8.0,2758.0]|[2.0592043430122886,4.479479832543546,3.471208875901355]   |\n",
      "|[152.0,8.0,2758.0]|[1.8092431221842074,4.479479832543546,3.471208875901355]   |\n",
      "|[104.0,8.0,472.0] |[1.2379031888628786,4.479479832543546,0.5940575016045828]  |\n",
      "|[104.0,8.0,460.0] |[1.2379031888628786,4.479479832543546,0.5789543447841273]  |\n",
      "|[147.0,8.0,440.0] |[1.749728545796569,4.479479832543546,0.5537824167500349]   |\n",
      "|[324.0,4.0,787.0] |[3.856544549918968,2.239739916271773,0.9905153681415396]   |\n",
      "|[304.0,4.0,757.0] |[3.6184862443684147,2.239739916271773,0.952757476090401]   |\n",
      "|[339.0,4.0,711.0] |[4.035088279081884,2.239739916271773,0.8948620416119881]   |\n",
      "+------------------+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "escala = StandardScaler(inputCol=\"Matriz\", outputCol=\"standard\", withStd=True, withMean=False)\n",
    "modelo = escala.fit(carros_vetor)\n",
    "carrosstandard = modelo.transform(carros_vetor)\n",
    "\n",
    "carrosstandard.select(\"Matriz\",\"standard\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756e324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
