{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d1e619-c88f-4945-b574-5e613d5cc510",
   "metadata": {},
   "source": [
    "### Prof. Fernando Amaral - www.eia.ai\n",
    "### Contribuição: Adriano Santos\n",
    "#### <strong><font color=orange>Machine Learning com Spark</font></strong>\n",
    "## <strong>Importação</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78adb6a0",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **findspark** e o **pyspark** para importar e configurar o ambiente do Spark em Python. A biblioteca **findspark** é responsável por localizar a instalação do Spark no sistema, enquanto o **pyspark** permite a utilização das funcionalidades do Spark diretamente em Python. Em seguida, é importada a classe **SparkSession** do módulo **pyspark.sql** para criar uma sessão do Spark, que é essencial para interagir com os dados no ambiente Spark. A função **findspark.init()** inicializa o Spark, garantindo que ele possa ser encontrado pelo Python. Por fim, a variável **spark** é definida como uma instância da classe **SparkSession**, utilizando o método **builder** para configurar a sessão com o nome da aplicação (\"importacao\") e o método **getOrCreate()** para obter a sessão existente ou criar uma nova, se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0305a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"importacao\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dea305",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo definido um esquema de dados, representado pela string **arqschema**, que especifica os tipos de cada coluna de um conjunto de dados, incluindo id, nome, status, cidade, vendas e data. Em seguida, é feita a leitura de um arquivo CSV, localizado no diretório \"../despachantes.csv\", utilizando a função **spark.read.csv**, onde o parâmetro **header=False** indica que o arquivo não possui cabeçalho e o parâmetro **schema=arqschema** especifica o esquema a ser aplicado. Por fim, é exibido um resumo das primeiras cinco linhas do conjunto de dados **despachantes** por meio da função **show(5)**. Esse trecho de código utiliza o framework Apache Spark para processamento de big data em Python. Por fim, é mostrado o esquema do DataFrame despachantes utilizando a função schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f112d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+------+----------+\n",
      "| id|               nome|status|       cidade|vendas|      data|\n",
      "+---+-------------------+------+-------------+------+----------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
      "+---+-------------------+------+-------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('nome', StringType(), True), StructField('status', StringType(), True), StructField('cidade', StringType(), True), StructField('vendas', IntegerType(), True), StructField('data', DateType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arqschema = \"id INT, nome STRING, status STRING, cidade STRING, vendas INT, data DATE\"\n",
    "despachantes = spark.read.csv(\"../despachantes.csv\", header=False, schema=arqschema)\n",
    "\n",
    "despachantes.show(5)\n",
    "despachantes.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956d1b2",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **Spark** para carregar um arquivo CSV chamado \"despachantes.csv\" e atribuí-lo à variável **desp_autoschema**. A função **load** é empregada para carregar o arquivo, especificando o formato como CSV, indicando que não há cabeçalho (header=False) e permitindo que o Spark infira o esquema dos dados (inferSchema=True). O parâmetro **sep** define a vírgula como delimitador de colunas. Em seguida, é chamado o método **show(5)** para exibir as primeiras cinco linhas do DataFrame resultante, fornecendo uma visualização inicial dos dados carregados. Por fim, é mostrado o esquema do DataFrame despachantes utilizando a função schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef70ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('_c0', IntegerType(), True), StructField('_c1', StringType(), True), StructField('_c2', StringType(), True), StructField('_c3', StringType(), True), StructField('_c4', IntegerType(), True), StructField('_c5', DateType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desp_autoschema = spark.read.load(\"../despachantes.csv\", format=\"csv\", header=False, inferSchema=True, sep=\",\" )\n",
    "\n",
    "desp_autoschema.show(5)\n",
    "desp_autoschema.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f0568",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o **Spark** para carregar dados de um arquivo no formato Parquet. A variável **par** é utilizada para armazenar os dados carregados através da função **spark.read.format(\"parquet\").load(\"../despachantes.parquet\")**, onde **spark** é o contexto do Spark que permite operações de leitura e manipulação de dados, **read** é uma função para ler dados, **format(\"parquet\")** especifica o formato do arquivo a ser lido como Parquet, e **load(\"../despachantes.parquet\")** carrega os dados do arquivo localizado no diretório \"../despachantes.parquet\". Em seguida, a função **show()** é chamada na variável **par** para exibir os dados carregados na saída padrão, mostrando uma visualização dos dados para análise e inspeção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa7f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "par = spark.read.format(\"parquet\").load(\"../despachantes.parquet\")\n",
    "par.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65df328",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo utilizado o Python com a biblioteca **Spark** para ler um arquivo JSON chamado \"despachantes.json\". A função **spark.read.format(\"json\")** é utilizada para indicar o formato do arquivo que será lido, que neste caso é JSON. Em seguida, a função **load(\"../despachantes.json\")** é chamada para carregar o arquivo JSON especificado no diretório \"../despachantes.json\". Por fim, a função **show()** é usada para exibir os dados do arquivo JSON na saída padrão, possibilitando uma visualização rápida do conteúdo do arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ca313c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---+-------------------+------+------+\n",
      "|       cidade|       data| id|               nome|status|vendas|\n",
      "+-------------+-----------+---+-------------------+------+------+\n",
      "|  Santa Maria| 2020-08-11|  1|   Carminda Pestana| Ativo|    23|\n",
      "|Novo Hamburgo| 2020-03-05|  2|    Deolinda Vilela| Ativo|    34|\n",
      "| Porto Alegre| 2020-02-05|  3|   Emídio Dornelles| Ativo|    34|\n",
      "| Porto Alegre| 2020-02-05|  4|Felisbela Dornelles| Ativo|    36|\n",
      "| Porto Alegre| 2020-02-05|  5|     Graça Ornellas| Ativo|    12|\n",
      "| Porto Alegre| 2019-01-05|  6|   Matilde Rebouças| Ativo|    22|\n",
      "|  Santa Maria| 2019-10-05|  7|    Noêmia   Orriça| Ativo|    45|\n",
      "| Porto Alegre| 2020-03-05|  8|      Roque Vásquez| Ativo|    65|\n",
      "| Porto Alegre| 2018-05-05|  9|      Uriel Queiroz| Ativo|    54|\n",
      "| Porto Alegre| 2020-09-05| 10|   Viviana Sequeira| Ativo|     0|\n",
      "+-------------+-----------+---+-------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "js = spark.read.format(\"json\").load(\"../despachantes.json\")\n",
    "js.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ae26b",
   "metadata": {},
   "source": [
    "No código a seguir, está sendo feita a leitura de um arquivo no formato ORC utilizando a biblioteca **Spark**. Primeiramente, é feita a importação da biblioteca **spark**, e em seguida, o arquivo \"despachantes.orc\" é carregado utilizando a função **read** com o método **format** especificado como \"orc\". O caminho do arquivo é relativo, utilizando o diretório \"../\" para acessar o diretório pai. Por fim, a função **show** é utilizada para exibir os dados carregados do arquivo ORC, possibilitando uma visualização dos dados no console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21d83d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orc = spark.read.format(\"orc\").load(\"../despachantes.orc\")\n",
    "orc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc61809",
   "metadata": {},
   "source": [
    "### <font color=orange><strong>O que podemos aprender como isso?</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5b6af",
   "metadata": {},
   "source": [
    "Essas informações abordam o uso do Apache Spark em Python para manipulação de dados em diferentes formatos, como CSV, Parquet, JSON e ORC. \n",
    "\n",
    "1. **Importação e Configuração do Ambiente Spark em Python:**\n",
    "   - Utiliza-se as bibliotecas **findspark** e **pyspark** para configurar o ambiente do Spark em Python.\n",
    "   - **findspark** localiza a instalação do Spark no sistema, enquanto **pyspark** permite a utilização das funcionalidades do Spark em Python.\n",
    "   - A classe **SparkSession** do módulo **pyspark.sql** é importada para criar uma sessão do Spark, essencial para interagir com os dados.\n",
    "   - A função **findspark.init()** inicializa o Spark, e a variável **spark** é definida como uma instância da classe **SparkSession**.\n",
    "\n",
    "2. **Leitura de Dados em Formato CSV e Especificação de Esquema:**\n",
    "   - É definido um esquema de dados especificando os tipos de cada coluna.\n",
    "   - A leitura de um arquivo CSV é realizada com **spark.read.csv**, especificando o esquema e indicando que não há cabeçalho.\n",
    "   - As primeiras cinco linhas do conjunto de dados são exibidas com **show(5)**, e o esquema é mostrado com a função **schema**.\n",
    "\n",
    "3. **Leitura de Dados em Formato Parquet:**\n",
    "   - Utiliza-se **spark.read.format(\"parquet\").load()** para carregar dados de um arquivo Parquet.\n",
    "   - A função **show()** é chamada para exibir os dados carregados.\n",
    "\n",
    "4. **Leitura de Dados em Formato JSON:**\n",
    "   - É feita a leitura de um arquivo JSON com **spark.read.format(\"json\").load()**.\n",
    "   - Os dados são exibidos com **show()**.\n",
    "\n",
    "5. **Leitura de Dados em Formato ORC:**\n",
    "   - O arquivo ORC é carregado com **read.format(\"orc\")**, e os dados são exibidos utilizando **show()**.\n",
    "\n",
    "Esses exemplos demonstram diferentes maneiras de carregar e visualizar dados com o Spark em Python, oferecendo flexibilidade para lidar com diversos formatos de arquivo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
