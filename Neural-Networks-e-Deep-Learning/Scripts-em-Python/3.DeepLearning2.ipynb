{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formação Cientista de Dados - Fernando Amaral e Jones Granatyr\n",
    "# Contribuição: Adriano Santos\n",
    "\n",
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as Bibliotecas Nescessária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código abaixo é usado para construir um modelo de aprendizado de máquina que pode prever resultados com base em dados. Ele utiliza bibliotecas como pandas para manipular dados, keras para construir a estrutura do modelo, e sklearn para preparar os dados. O objetivo principal é treinar um modelo que possa fazer previsões com base em um conjunto de dados, o que pode ser útil em várias aplicações, como prever vendas ou diagnósticos médicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o Dataset Credit2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código, estamos basicamente carregando um conjunto de dados chamado \"Credit2.csv\". Primeiro, definimos o caminho do arquivo onde ele está localizado e especificamos que o separador entre as informações é um ponto e vírgula. Em seguida, usamos uma biblioteca chamada \"pandas\" para ler esse arquivo e armazenar os dados no objeto \"dataset\". Por fim, exibimos as 10 primeiras linhas desse conjunto de dados para dar uma rápida olhada em seu conteúdo. É uma etapa inicial comum na análise de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>checking_status</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>residence_since</th>\n",
       "      <th>age</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;0</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>6</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0&lt;=X&lt;200</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>48</td>\n",
       "      <td>5951</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>no checking</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>12</td>\n",
       "      <td>2096</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;0</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>42</td>\n",
       "      <td>7882</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>&lt;0</td>\n",
       "      <td>delayed previously</td>\n",
       "      <td>24</td>\n",
       "      <td>4870</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>no checking</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>36</td>\n",
       "      <td>9055</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>no checking</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>24</td>\n",
       "      <td>2835</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0&lt;=X&lt;200</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>36</td>\n",
       "      <td>6948</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>no checking</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>12</td>\n",
       "      <td>3059</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0&lt;=X&lt;200</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>30</td>\n",
       "      <td>5234</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID checking_status                  credit_history  duration  \\\n",
       "0   1              <0  critical/other existing credit         6   \n",
       "1   2        0<=X<200                   existing paid        48   \n",
       "2   3     no checking  critical/other existing credit        12   \n",
       "3   4              <0                   existing paid        42   \n",
       "4   5              <0              delayed previously        24   \n",
       "5   6     no checking                   existing paid        36   \n",
       "6   7     no checking                   existing paid        24   \n",
       "7   8        0<=X<200                   existing paid        36   \n",
       "8   9     no checking                   existing paid        12   \n",
       "9  10        0<=X<200  critical/other existing credit        30   \n",
       "\n",
       "   credit_amount  installment_commitment  residence_since  age  \\\n",
       "0           1169                       4                4   67   \n",
       "1           5951                       2                2   22   \n",
       "2           2096                       2                3   49   \n",
       "3           7882                       2                4   45   \n",
       "4           4870                       3                4   53   \n",
       "5           9055                       2                4   35   \n",
       "6           2835                       3                4   53   \n",
       "7           6948                       2                2   35   \n",
       "8           3059                       2                4   61   \n",
       "9           5234                       4                2   28   \n",
       "\n",
       "   existing_credits  num_dependents class  \n",
       "0                 2               1  good  \n",
       "1                 1               1   bad  \n",
       "2                 1               2  good  \n",
       "3                 1               2  good  \n",
       "4                 2               2   bad  \n",
       "5                 1               2  good  \n",
       "6                 1               1  good  \n",
       "7                 1               1  good  \n",
       "8                 1               1  good  \n",
       "9                 2               1   bad  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caminho_do_arquivo = \"../dados/Credit2.csv\"\n",
    "separador = \";\"\n",
    "\n",
    "dataset = pd.read_csv(caminho_do_arquivo, sep=separador)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando as Variáveis de Entrada e de Saída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste código abaixo, estamos realizando duas etapas: primeiro, estamos separando as variáveis de entrada (que contêm informações relevantes) e a variável de saída (que é o resultado que queremos prever) de um conjunto de dados chamado 'dataset'. Em seguida, exibimos as variáveis de entrada para visualização. Isso é útil para entender quais informações estamos usando para fazer previsões ou análises, tornando o processo mais claro para quem está acompanhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<0' 'critical/other existing credit' 6 ... 67 2 1]\n",
      " ['0<=X<200' 'existing paid' 48 ... 22 1 1]\n",
      " ['no checking' 'critical/other existing credit' 12 ... 49 1 2]\n",
      " ...\n",
      " ['no checking' 'existing paid' 12 ... 38 1 1]\n",
      " ['<0' 'existing paid' 45 ... 23 1 1]\n",
      " ['0<=X<200' 'critical/other existing credit' 45 ... 27 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Separando as variáveis de entrada (features) e a variável de saída (target)\n",
    "features = dataset.iloc[:, 1:10].values\n",
    "target = dataset.iloc[:, 10].values\n",
    "\n",
    "# Exibindo as features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando Coluna de um Conjunto de Dados em Número"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código, estamos transformando a coluna 'checking_status' de um conjunto de dados em números para que o computador possa entendê-la. Isso é feito usando uma técnica chamada LabelEncoder, que associa cada categoria única da coluna a um número. Em seguida, o código exibe as features (características) do conjunto de dados após essa codificação, permitindo que você veja como as categorias foram convertidas em números. Isso é útil para análise de dados e machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 'critical/other existing credit' 6 ... 67 2 1]\n",
      " [0 'existing paid' 48 ... 22 1 1]\n",
      " [3 'critical/other existing credit' 12 ... 49 1 2]\n",
      " ...\n",
      " [3 'existing paid' 12 ... 38 1 1]\n",
      " [1 'existing paid' 45 ... 23 1 1]\n",
      " [0 'critical/other existing credit' 45 ... 27 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Codificação da coluna 'checking_status' usando LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "features[:, 0] = label_encoder.fit_transform(features[:, 0])\n",
    "\n",
    "# Exibição das features após a codificação\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando uma Coluna em Múltiplas Colunas Binárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código abaixo é usado para transformar uma coluna chamada \"credit_history\" em múltiplas colunas binárias (0 ou 1), onde cada coluna representa uma categoria única dentro dessa coluna original. Isso é útil para preparar dados para análise ou modelagem de machine learning. O resultado final é uma matriz onde cada linha corresponde a um exemplo de dados e cada coluna representa a presença ou ausência de uma categoria específica de histórico de crédito, tornando os dados mais compreensíveis e úteis para análise ou modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 ... 67 2 1]\n",
      " [0.0 0.0 0.0 ... 22 1 1]\n",
      " [0.0 1.0 0.0 ... 49 1 2]\n",
      " ...\n",
      " [0.0 0.0 0.0 ... 38 1 1]\n",
      " [0.0 0.0 0.0 ... 23 1 1]\n",
      " [0.0 1.0 0.0 ... 27 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encoder coluna credit_history\n",
    "#deve adicionar 5 colunas\n",
    "onehotencoder = make_column_transformer(\n",
    "    (OneHotEncoder(categories='auto', sparse_output=False), [1]), \n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "x_transformado = onehotencoder.fit_transform(features)\n",
    "print(x_transformado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo a Primeira Coluna das Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código simples, estamos trabalhando com um conjunto de dados e removendo a primeira coluna das características (variáveis) para evitar um problema comum chamado \"dummy variable trap\". Isso é feito para evitar multicolinearidade, que pode atrapalhar análises estatísticas. A variável X_removida agora contém as características restantes após a remoção da primeira coluna, e estamos imprimindo esses dados para visualização. É uma etapa comum em análises de dados para melhorar a qualidade dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['critical/other existing credit' 6 1169 ... 67 2 1]\n",
      " ['existing paid' 48 5951 ... 22 1 1]\n",
      " ['critical/other existing credit' 12 2096 ... 49 1 2]\n",
      " ...\n",
      " ['existing paid' 12 804 ... 38 1 1]\n",
      " ['existing paid' 45 1845 ... 23 1 1]\n",
      " ['critical/other existing credit' 45 4576 ... 27 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Removemos a primeira coluna das features para evitar a dummy variable trap\n",
    "X_removida = features[:, 1:]\n",
    "print(X_removida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificando Dados Categóricos em Números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código abaixo está sendo usado para codificar dados categóricos em números. Ele usa a classe 'LabelEncoder' para fazer essa transformação. Os dados originais estão armazenados na variável 'y', e o código os transforma em uma nova representação numérica, armazenada na variável 'y_encoded'. Finalmente, o código imprime os dados codificados para que possam ser facilmente usados em análises ou modelos de machine learning. É uma maneira de tornar dados com categorias mais compreensíveis para computadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0\n",
      " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
      " 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1\n",
      " 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1\n",
      " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
      " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
      " 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
      " 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar o LabelEncoder aos dados de destino (variável y)\n",
    "labelencoder_Y = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Retornar os dados codificados\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo os Dados em 2 Grupos: Treino e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código, estamos dividindo nossos dados em dois grupos: um para treinar um modelo e outro para testá-lo. Isso nos ajuda a avaliar o quão bem nosso modelo funciona. Em seguida, estamos mostrando o tamanho desses grupos, ou seja, quantos exemplos temos em cada um, para ter uma ideia de quão grande é nossa amostra de treinamento e teste. Essa divisão e visualização são passos importantes no processo de construção e avaliação de modelos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: 800 amostras\n",
      "Tamanho do conjunto de teste: 200 amostras\n"
     ]
    }
   ],
   "source": [
    "# Dividir o conjunto de dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_transformado, y_encoded, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Imprimir o tamanho dos conjuntos de treino e teste\n",
    "print(f\"Tamanho do conjunto de treino: {len(X_train)} amostras\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(X_test)} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados para Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código abaixo, estamos preparando dados para análise. Primeiro, criamos uma ferramenta chamada \"scaler\" que ajuda a tornar nossos dados comparáveis, como medir algo em diferentes escalas. Em seguida, aplicamos esse \"scaler\" aos nossos dados de treinamento, garantindo que eles estejam todos na mesma escala. Depois, aplicamos o mesmo \"scaler\" aos dados de teste, para que possamos usá-los de maneira consistente. Por fim, imprimimos os dados de teste que foram ajustados para essa escala padronizada. Isso é importante em tarefas de análise de dados para obter resultados precisos e comparáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22331316 -0.65270587 -0.30966177 ... -0.50870719 -0.71596668\n",
      "  -0.42214126]\n",
      " [-0.22331316 -0.65270587 -0.30966177 ... -0.85315557 -0.71596668\n",
      "   2.36887531]\n",
      " [-0.22331316 -0.65270587 -0.30966177 ...  0.61075002 -0.71596668\n",
      "  -0.42214126]\n",
      " ...\n",
      " [-0.22331316  1.53208366 -0.30966177 ...  0.26630165  1.04100677\n",
      "  -0.42214126]\n",
      " [-0.22331316 -0.65270587  3.22932987 ...  0.52463793  4.55495365\n",
      "  -0.42214126]\n",
      " [-0.22331316 -0.65270587 -0.30966177 ...  0.52463793 -0.71596668\n",
      "   2.36887531]]\n"
     ]
    }
   ],
   "source": [
    "# Inicialize o StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajuste e transforme os dados de treinamento\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apenas transforme os dados de teste usando o mesmo scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Imprimindo os dados de teste padronizados\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando e Treinando uma Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código abaixo cria e treina uma rede neural simples para resolver um problema de classificação binária. Ele define um modelo sequencial com três camadas: duas camadas ocultas com ativação ReLU (função de ativação) e uma camada de saída com ativação sigmoid. A rede é compilada com o otimizador 'adam' e a função de perda 'binary_crossentropy'. Em seguida, é treinada com os dados de treinamento (X_train e y_train) em mini lotes de 10 exemplos, durante 100 épocas, ajustando os pesos da rede para fazer previsões precisas. Essa rede neural é usada principalmente para tarefas de classificação binária, como prever se algo é verdadeiro ou falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - 2s 3ms/step - loss: 0.6853 - accuracy: 0.6950\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6366 - accuracy: 0.6975\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5555 - accuracy: 0.6975\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5313 - accuracy: 0.6975\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5237 - accuracy: 0.7000\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5205 - accuracy: 0.7325\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5180 - accuracy: 0.7525\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5159 - accuracy: 0.7575\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.7538\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7600\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5112 - accuracy: 0.7638\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5089 - accuracy: 0.7613\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5081 - accuracy: 0.7613\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5068 - accuracy: 0.7638\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5053 - accuracy: 0.7613\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.7663\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5032 - accuracy: 0.7638\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5027 - accuracy: 0.7675\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5009 - accuracy: 0.7650\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5006 - accuracy: 0.7688\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7650\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4995 - accuracy: 0.7638\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4992 - accuracy: 0.7713\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4986 - accuracy: 0.7650\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4974 - accuracy: 0.7675\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4966 - accuracy: 0.7688\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4962 - accuracy: 0.7688\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4957 - accuracy: 0.7688\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4946 - accuracy: 0.7650\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4950 - accuracy: 0.7650\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 0.7663\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4934 - accuracy: 0.7713\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4933 - accuracy: 0.7675\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4918 - accuracy: 0.7663\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.4909 - accuracy: 0.7688\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.4905 - accuracy: 0.7688\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4908 - accuracy: 0.7675\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4901 - accuracy: 0.7663\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4888 - accuracy: 0.7713\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4890 - accuracy: 0.7663\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4878 - accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4872 - accuracy: 0.7650\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4873 - accuracy: 0.7675\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4866 - accuracy: 0.7688\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4858 - accuracy: 0.7663\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4859 - accuracy: 0.7650\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4855 - accuracy: 0.7650\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.4852 - accuracy: 0.7638\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4849 - accuracy: 0.7613\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4848 - accuracy: 0.7638\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4843 - accuracy: 0.7625\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4835 - accuracy: 0.7663\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4837 - accuracy: 0.7663\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.4835 - accuracy: 0.7625\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4829 - accuracy: 0.7663\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4830 - accuracy: 0.7638\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4827 - accuracy: 0.7675\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4818 - accuracy: 0.7675\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4814 - accuracy: 0.7700\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.4813 - accuracy: 0.7663\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.4815 - accuracy: 0.7688\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4805 - accuracy: 0.7725\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4809 - accuracy: 0.7688\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4799 - accuracy: 0.7675\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.7700\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4798 - accuracy: 0.7700\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4796 - accuracy: 0.7713\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4795 - accuracy: 0.7688\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.7713\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4789 - accuracy: 0.7675\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4783 - accuracy: 0.7725\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.7713\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4780 - accuracy: 0.7650\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4768 - accuracy: 0.7700\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.4768 - accuracy: 0.7713\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.4775 - accuracy: 0.7663\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4764 - accuracy: 0.7713\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4756 - accuracy: 0.7688\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4758 - accuracy: 0.7725\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7725\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4748 - accuracy: 0.7700\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4753 - accuracy: 0.7663\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4750 - accuracy: 0.7688\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4745 - accuracy: 0.7688\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7663\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4745 - accuracy: 0.7713\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4738 - accuracy: 0.7713\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.7738\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4734 - accuracy: 0.7663\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4739 - accuracy: 0.7700\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4726 - accuracy: 0.7700\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.4722 - accuracy: 0.7725\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.4715 - accuracy: 0.7738\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.4715 - accuracy: 0.7738\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4716 - accuracy: 0.7713\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4710 - accuracy: 0.7763\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4713 - accuracy: 0.7713\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4706 - accuracy: 0.7713\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.7750\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.4702 - accuracy: 0.7725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16c87aec450>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação do modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Adição das camadas ocultas com ativação ReLU\n",
    "model.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=13))\n",
    "model.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# Adição da camada de saída com ativação sigmoid\n",
    "model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compilação do modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinamento do modelo\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando um Modelo para Fazer Previsões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código simples, estamos usando um modelo para fazer previsões com base em algum conjunto de dados (X_test). Em seguida, definimos um limite (limiar) de 0,5 para classificar essas previsões em duas categorias, verdadeiro ou falso. Finalmente, exibimos essas previsões binárias, ou seja, se são verdadeiras (verdadeiro) ou falsas (falso), para ajudar na tomada de decisões. É uma técnica comum em análise de dados e aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 3ms/step\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "# Fazendo previsões com o modelo\n",
    "previsoes = model.predict(X_test)\n",
    "\n",
    "# Definindo um limiar (threshold) para classificação binária\n",
    "limiar = 0.5\n",
    "\n",
    "# Aplicando o limiar para obter as previsões binárias\n",
    "previsoes_binarias = (previsoes > limiar)\n",
    "\n",
    "# Exibindo as previsões binárias\n",
    "print(previsoes_binarias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando e Exibindo a Matriz de Confusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código abaixo calcula e exibe a chamada \"Matriz de Confusão\" em um problema de classificação. Essa matriz mostra como um modelo de machine learning se saiu ao prever resultados, comparando as previsões feitas (variável 'previsoes_binarias') com os valores reais (variável 'y_test'). É uma ferramenta importante para avaliar o desempenho do modelo, ajudando a identificar quantos acertos e erros ele teve em suas previsões, o que é útil para entender a qualidade do modelo em termos de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusão:\n",
      "\n",
      "[[ 19  39]\n",
      " [ 19 123]]\n"
     ]
    }
   ],
   "source": [
    "# Calcular a matriz de confusão\n",
    "matriz_confusao = confusion_matrix(y_test, previsoes_binarias)\n",
    "\n",
    "# Exibir a matriz de confusão\n",
    "print(\"Matriz de Confusão:\\n\")\n",
    "print(matriz_confusao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
